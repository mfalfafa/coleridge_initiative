{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74a0678",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-25T04:43:05.471393Z",
     "iopub.status.busy": "2021-09-25T04:43:05.470569Z",
     "iopub.status.idle": "2021-09-25T04:44:36.037752Z",
     "shell.execute_reply": "2021-09-25T04:44:36.036390Z",
     "shell.execute_reply.started": "2021-09-25T01:43:54.176876Z"
    },
    "papermill": {
     "duration": 90.688119,
     "end_time": "2021-09-25T04:44:36.037925",
     "exception": false,
     "start_time": "2021-09-25T04:43:05.349806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.12.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.5)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\r\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.0.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.17)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.8.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.6.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\r\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.5.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# additional python packages\n",
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "!pip install -q ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "!pip install -q ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "!pip install -q ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251a548b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:36.117800Z",
     "iopub.status.busy": "2021-09-25T04:44:36.117111Z",
     "iopub.status.idle": "2021-09-25T04:44:48.722674Z",
     "shell.execute_reply": "2021-09-25T04:44:48.723257Z",
     "shell.execute_reply.started": "2021-09-25T01:45:25.386897Z"
    },
    "papermill": {
     "duration": 12.649568,
     "end_time": "2021-09-25T04:44:48.723489",
     "exception": false,
     "start_time": "2021-09-25T04:44:36.073921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 04:44:42.585096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import simplejson\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "# dataset manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\n",
    "AutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "from typing import List\n",
    "import string\n",
    "from functools import partial\n",
    "\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import gc\n",
    "\n",
    "# tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# language preprocessing\n",
    "import nltk\n",
    "\n",
    "from typing import *\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# set seed\n",
    "sns.set()\n",
    "random.seed(123)\n",
    "np.random.seed(456)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print('packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd9f647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:48.797351Z",
     "iopub.status.busy": "2021-09-25T04:44:48.796561Z",
     "iopub.status.idle": "2021-09-25T04:44:48.809372Z",
     "shell.execute_reply": "2021-09-25T04:44:48.808978Z",
     "shell.execute_reply.started": "2021-09-25T01:45:37.882706Z"
    },
    "papermill": {
     "duration": 0.051387,
     "end_time": "2021-09-25T04:44:48.809533",
     "exception": false,
     "start_time": "2021-09-25T04:44:48.758146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this submission notebook will only be used to submit result\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "\n",
    "COMPUTE_CV = False\n",
    "\n",
    "if len(sample_submission)>4: COMPUTE_CV = False\n",
    "if COMPUTE_CV:\n",
    "    print('this submission notebook will compute CV score but commit notebook will not')\n",
    "else:\n",
    "    print('this submission notebook will only be used to submit result')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6435744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:48.881893Z",
     "iopub.status.busy": "2021-09-25T04:44:48.881223Z",
     "iopub.status.idle": "2021-09-25T04:44:48.995640Z",
     "shell.execute_reply": "2021-09-25T04:44:48.994683Z",
     "shell.execute_reply.started": "2021-09-25T01:45:37.898623Z"
    },
    "papermill": {
     "duration": 0.15237,
     "end_time": "2021-09-25T04:44:48.995799",
     "exception": false,
     "start_time": "2021-09-25T04:44:48.843429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "if COMPUTE_CV:\n",
    "    sample_submission = train\n",
    "    paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n",
    "    test_files_path = paper_test_folder\n",
    "else:\n",
    "    sample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "    paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\n",
    "    test_files_path = paper_test_folder\n",
    "\n",
    "adnl_govt_labels_path = '../input/bigger-govt-dataset-list/data_set_800.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fea86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.079374Z",
     "iopub.status.busy": "2021-09-25T04:44:49.078568Z",
     "iopub.status.idle": "2021-09-25T04:44:49.080939Z",
     "shell.execute_reply": "2021-09-25T04:44:49.081370Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.031077Z"
    },
    "papermill": {
     "duration": 0.048669,
     "end_time": "2021-09-25T04:44:49.081524",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.032855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SAMPLE = 0\n",
    "\n",
    "train = train[:MAX_SAMPLE]\n",
    "\n",
    "paper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n",
    "papers = {}\n",
    "for paper_id in train['Id'].unique():\n",
    "    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a346600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.154843Z",
     "iopub.status.busy": "2021-09-25T04:44:49.153961Z",
     "iopub.status.idle": "2021-09-25T04:44:49.201240Z",
     "shell.execute_reply": "2021-09-25T04:44:49.201810Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.044567Z"
    },
    "papermill": {
     "duration": 0.086543,
     "end_time": "2021-09-25T04:44:49.201994",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.115451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 99.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for paper_id in tqdm(sample_submission['Id']):\n",
    "    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7ffa8",
   "metadata": {
    "papermill": {
     "duration": 0.035114,
     "end_time": "2021-09-25T04:44:49.273079",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.237965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Additional goverent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929e0c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.347974Z",
     "iopub.status.busy": "2021-09-25T04:44:49.346549Z",
     "iopub.status.idle": "2021-09-25T04:44:49.348713Z",
     "shell.execute_reply": "2021-09-25T04:44:49.349128Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.090109Z"
    },
    "papermill": {
     "duration": 0.041151,
     "end_time": "2021-09-25T04:44:49.349245",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.308094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6752cf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.424445Z",
     "iopub.status.busy": "2021-09-25T04:44:49.423615Z",
     "iopub.status.idle": "2021-09-25T04:44:49.498238Z",
     "shell.execute_reply": "2021-09-25T04:44:49.497783Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.097182Z"
    },
    "papermill": {
     "duration": 0.114369,
     "end_time": "2021-09-25T04:44:49.498360",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.383991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp3 = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "\n",
    "tmp3_ = [x for x in tmp3['cleaned_label'].unique() if len(str(x).split()) > 0]\n",
    "tmp3_ += [x for x in tmp3['dataset_title'].unique()]\n",
    "tmp3 = [clean_text(x) for x in np.unique(tmp3_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bdcc11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.573437Z",
     "iopub.status.busy": "2021-09-25T04:44:49.572853Z",
     "iopub.status.idle": "2021-09-25T04:44:49.761136Z",
     "shell.execute_reply": "2021-09-25T04:44:49.760325Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.179018Z"
    },
    "papermill": {
     "duration": 0.227957,
     "end_time": "2021-09-25T04:44:49.761261",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.533304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010 nielsen homescan survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019 ncov complete genome sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019 ncov genome sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019 ncov genome sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>about the workshop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title\n",
       "0         2010 nielsen homescan survey\n",
       "1  2019 ncov complete genome sequences\n",
       "2            2019 ncov genome sequence\n",
       "3           2019 ncov genome sequences\n",
       "4                   about the workshop"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp8 = pd.read_csv('../input/ci-ext-datasets-found-in-train-v2/train_ext_data.csv')\n",
    "tmp8['ext_cleaned_label'] = tmp8['ext_cleaned_label'].apply(lambda x: x.split('|'))\n",
    "all_labels = []\n",
    "for labels in tmp8['ext_cleaned_label'].values:\n",
    "    for l in labels:\n",
    "        all_labels.append(l)\n",
    "tmp8 = list(np.unique(all_labels))\n",
    "tmp8 = pd.DataFrame(tmp8, columns=['title'])\n",
    "tmp8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41888b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:49.842386Z",
     "iopub.status.busy": "2021-09-25T04:44:49.840684Z",
     "iopub.status.idle": "2021-09-25T04:44:49.844757Z",
     "shell.execute_reply": "2021-09-25T04:44:49.844284Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.379562Z"
    },
    "papermill": {
     "duration": 0.04761,
     "end_time": "2021-09-25T04:44:49.844865",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.797255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "tmp8_ = []\n",
    "for l in tmp8['title'].values:\n",
    "    if l not in tmp3:\n",
    "        tmp8_.append(l)\n",
    "        \n",
    "print(len(tmp8_))\n",
    "tmp8_ = pd.DataFrame(tmp8_, columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7830694d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.011854Z",
     "iopub.status.busy": "2021-09-25T04:44:50.010219Z",
     "iopub.status.idle": "2021-09-25T04:44:50.015755Z",
     "shell.execute_reply": "2021-09-25T04:44:50.016279Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.388219Z"
    },
    "papermill": {
     "duration": 0.13582,
     "end_time": "2021-09-25T04:44:50.016482",
     "exception": false,
     "start_time": "2021-09-25T04:44:49.880662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "not_datasets = ['about', 'climatologists', 'control', 'exploration', 'defense', \n",
    "                'american community', 'american landscape', 'current population survey',\n",
    "                'gulf of maine', 'argonne national laboratory s greet', \n",
    "                'annual wholesale trade',\n",
    "                'bird conservation areas', 'bird incidental take', 'new housing', 'business patterns',\n",
    "                'create', 'federal aid to states', 'freedom of information act', 'fruit and vegetable prices',\n",
    "                'guidance navigation and control', 'high school and beyond', 'human resource management', \n",
    "                'housing unit estimates', 'international data base', 'labor market analysts', 'major land uses',\n",
    "                'mars exploration program', 'new residential construction', 'oxygen delivery system',\n",
    "                'pilot boarding areas', 'profiles in science', 'state fact sheets', 'summary of business',\n",
    "                'tsunamis general', 'virtual grower', # 0.620\n",
    "                \n",
    "                'advanced monthly', \n",
    "                'advanced telecommunications', \n",
    "                'agricultural productivity',\n",
    "                'annual survey', \n",
    "                'breeding bird', \n",
    "                'bridged race population estimates', \n",
    "                'building permits survey',\n",
    "                'census of governments', \n",
    "                'clinical laboratory', 'coastal energy facilities', \n",
    "                'commodity costs and returns',\n",
    "                'comprehensive environmental', 'county typology codes', 'delta cost project', \n",
    "                'endangered species act',\n",
    "                'energy policy act', \n",
    "                'fertilizer', 'geostationary', 'landfire', 'occupational projections', \n",
    "                'marine mammal protection act', \n",
    "                'meat price', 'medication therapy', 'mexican american', \n",
    "                'milk cost',\n",
    "                'animal health', 'weather', 'national environmental policy', 'national outbreak', 'natural amenities scale',\n",
    "                'office', 'services file', 'stores', 'right whale', 'shuttle radar', 'solar dynamics',\n",
    "                'business owners', 'expedition', 'usa'\n",
    "               ]\n",
    "for l in not_datasets:\n",
    "    tmp8_ = tmp8_[~tmp8_['title'].str.contains(l)]\n",
    "    \n",
    "tmp8_.loc[tmp8_['title'].str.contains('national assessment of educational progress'), 'title'] = 'national assessment of educational progress'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('national postsecondary student aid study'), 'title'] = 'national postsecondary student aid study'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('nursing home compare'), 'title'] = 'nursing home compare'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('private school universe survey'), 'title'] = 'private school universe survey'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('program for international student assessment'), 'title'] = 'program for international student assessment'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('progress in international reading literacy study'), 'title'] = 'progress in international reading literacy study'\n",
    "tmp8_.loc[tmp8_['title'].str.contains('schools and staffing survey'), 'title'] = 'schools and staffing survey'\n",
    "\n",
    "tmp8_ = list(tmp8_['title'].unique())\n",
    "print(len(tmp8_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fccd7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.098425Z",
     "iopub.status.busy": "2021-09-25T04:44:50.097651Z",
     "iopub.status.idle": "2021-09-25T04:44:50.107203Z",
     "shell.execute_reply": "2021-09-25T04:44:50.106573Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.531561Z"
    },
    "papermill": {
     "duration": 0.053336,
     "end_time": "2021-09-25T04:44:50.107348",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.054012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['2010 nielsen homescan survey',\n",
       "       '2019 ncov complete genome sequences', '2019 ncov genome sequence',\n",
       "       '2019 ncov genome sequences',\n",
       "       'accredited postsecondary institutions and programs 2013'],\n",
       "      dtype='<U128')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = np.unique(tmp3 + tmp8_)\n",
    "all_datasets = np.unique([clean_text(x) for x in all_datasets])\n",
    "print(len(all_datasets))\n",
    "all_datasets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3dd8e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.192431Z",
     "iopub.status.busy": "2021-09-25T04:44:50.191649Z",
     "iopub.status.idle": "2021-09-25T04:44:50.193579Z",
     "shell.execute_reply": "2021-09-25T04:44:50.194072Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.54944Z"
    },
    "papermill": {
     "duration": 0.049531,
     "end_time": "2021-09-25T04:44:50.194189",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.144658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_training_text(txt):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n",
    "\n",
    "def totally_clean_text(txt):\n",
    "    txt = clean_text(txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt\n",
    "\n",
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n",
    "    text - Sentence that needs to be cleaned\n",
    "    '''\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "    \n",
    "def read_json_pub(filename, train_data_path=paper_train_folder, output='text'):\n",
    "    json_path = os.path.join(train_data_path, (filename+'.json'))\n",
    "    headings = []\n",
    "    contents = []\n",
    "    combined = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_decode = json.load(f)\n",
    "        for data in json_decode:\n",
    "            headings.append(data.get('section_title'))\n",
    "            contents.append(data.get('text'))\n",
    "            combined.append(data.get('section_title'))\n",
    "            combined.append(data.get('text'))\n",
    "    \n",
    "    all_headings = ' '.join(headings)\n",
    "    all_contents = ' '.join(contents)\n",
    "    all_data = '. '.join(combined)\n",
    "    \n",
    "    if output == 'text':\n",
    "        return all_contents\n",
    "    elif output == 'head':\n",
    "        return all_headings\n",
    "    else:\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ed2b4",
   "metadata": {
    "papermill": {
     "duration": 0.035831,
     "end_time": "2021-09-25T04:44:50.265909",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.230078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Literal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7191896f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.345360Z",
     "iopub.status.busy": "2021-09-25T04:44:50.344531Z",
     "iopub.status.idle": "2021-09-25T04:44:50.472990Z",
     "shell.execute_reply": "2021-09-25T04:44:50.472381Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.566071Z"
    },
    "papermill": {
     "duration": 0.171107,
     "end_time": "2021-09-25T04:44:50.473133",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.302026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 34.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adni|alzheimer s disease neuroimaging initiative adni|cardiovascular health study chs',\n",
       " 'common core of data|integrated postsecondary education data system|nces common core of data|oecd s online education database|program for international student assessment|progress in international reading literacy study|schools and staffing survey|trends in international mathematics and science study',\n",
       " 'noaa storm surge inundation|north carolina emergency management spatial data download|sea lake and overland surges from hurricanes|sea lake and overland surges from hurricanes slosh basin models|slosh model',\n",
       " '2010 nielsen homescan survey|food access research atlas|rural urban continuum codes']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = []\n",
    "to_append = []\n",
    "for index, row in tqdm(sample_submission.iterrows()):\n",
    "    to_append = [row['Id'],'']\n",
    "    large_string = str(read_json_pub(row['Id'], test_files_path))\n",
    "    clean_string = text_cleaning(large_string)\n",
    "    for row2 in all_datasets:\n",
    "        query_string = str(row2)\n",
    "        if query_string in clean_string:\n",
    "            if to_append[1] != '' and clean_text(query_string) not in to_append[1]:\n",
    "                to_append[1] = to_append[1] + '|' + clean_text(query_string)\n",
    "            if to_append[1] == '':\n",
    "                to_append[1] = clean_text(query_string)\n",
    "    literal_preds.append(*to_append[1:])\n",
    "literal_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d73506",
   "metadata": {
    "papermill": {
     "duration": 0.037324,
     "end_time": "2021-09-25T04:44:50.548178",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.510854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Masked Dataset Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dabfeb51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.629093Z",
     "iopub.status.busy": "2021-09-25T04:44:50.628280Z",
     "iopub.status.idle": "2021-09-25T04:44:50.631289Z",
     "shell.execute_reply": "2021-09-25T04:44:50.630819Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.702027Z"
    },
    "papermill": {
     "duration": 0.045861,
     "end_time": "2021-09-25T04:44:50.631397",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.585536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiple model\n",
    "PRETRAINED_PATH = [\n",
    "    '../input/coleridge-mlm-model/output-mlm/checkpoint-60000', #bert base cased 1\n",
    "    '../input/d/mfalfafa/ci-bert-base-cased-mlm/output-mlm/checkpoint-60000',\n",
    "]\n",
    "TOKENIZER_PATH = [\n",
    "    '../input/coleridge-mlm-model/model_tokenizer',\n",
    "    '../input/d/mfalfafa/ci-bert-base-cased-mlm/model_tokenizer',\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "OVERLAP = 20\n",
    "\n",
    "PREDICT_BATCH = 32 # a higher value requires higher GPU memory usage\n",
    "\n",
    "DATASET_SYMBOL = '$' # this symbol represents a dataset name\n",
    "NONDATA_SYMBOL = '#' # this symbol represents a non-dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335ad07",
   "metadata": {
    "papermill": {
     "duration": 0.037322,
     "end_time": "2021-09-25T04:44:50.706567",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.669245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tranform data into MLM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c664e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.787172Z",
     "iopub.status.busy": "2021-09-25T04:44:50.786622Z",
     "iopub.status.idle": "2021-09-25T04:44:50.789119Z",
     "shell.execute_reply": "2021-09-25T04:44:50.789623Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.7099Z"
    },
    "papermill": {
     "duration": 0.045552,
     "end_time": "2021-09-25T04:44:50.789774",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.744222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mlm_model(TOKENIZER_PATH, PRETRAINED_PATH):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(PRETRAINED_PATH)\n",
    "\n",
    "    mlm = pipeline(\n",
    "        'fill-mask', \n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    return tokenizer, model, mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b579e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.878723Z",
     "iopub.status.busy": "2021-09-25T04:44:50.878084Z",
     "iopub.status.idle": "2021-09-25T04:44:50.880271Z",
     "shell.execute_reply": "2021-09-25T04:44:50.880718Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.720058Z"
    },
    "papermill": {
     "duration": 0.053714,
     "end_time": "2021-09-25T04:44:50.880842",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.827128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = s1.split(\" \")\n",
    "    l2 = s2.split(\" \")    \n",
    "    intersection = len(list(set(l1).intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def clean_paper_sentence(s):\n",
    "    \"\"\"\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Sentences that have more than MAX_LENGTH words will be split\n",
    "    into multiple sentences with overlappings.\n",
    "    \"\"\"\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\n",
    "def find_mask_candidates(sentence):\n",
    "    \"\"\"\n",
    "    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n",
    "    A candidate should be a continuous sequence of at least 2 words, \n",
    "    each of these words either has the first letter in uppercase or is one of\n",
    "    the connection words ($connection_tokens). Furthermore, the connection \n",
    "    tokens are not allowed to appear at the beginning and the end of the\n",
    "    sequence.\n",
    "    \"\"\"\n",
    "    def candidate_qualified(words):\n",
    "        while len(words) and words[0].lower() in connection_tokens:\n",
    "            words = words[1:]\n",
    "        while len(words) and words[-1].lower() in connection_tokens:\n",
    "            words = words[:-1]\n",
    "        \n",
    "        return len(words) >= 2\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    phrase_start, phrase_end = -1, -1\n",
    "    for id in range(1, len(sentence)):\n",
    "        word = sentence[id]\n",
    "        if word[0].isupper() or word in connection_tokens:\n",
    "            if phrase_start == -1:\n",
    "                phrase_start = phrase_end = id\n",
    "            else:\n",
    "                phrase_end = id\n",
    "        else:\n",
    "            if phrase_start != -1:\n",
    "                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n",
    "                    candidates.append((phrase_start, phrase_end))\n",
    "                phrase_start = phrase_end = -1\n",
    "    \n",
    "    if phrase_start != -1:\n",
    "        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n",
    "            candidates.append((phrase_start, phrase_end))\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1fb8e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:50.963864Z",
     "iopub.status.busy": "2021-09-25T04:44:50.963300Z",
     "iopub.status.idle": "2021-09-25T04:44:50.965821Z",
     "shell.execute_reply": "2021-09-25T04:44:50.966180Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.737741Z"
    },
    "papermill": {
     "duration": 0.04803,
     "end_time": "2021-09-25T04:44:50.966297",
     "exception": false,
     "start_time": "2021-09-25T04:44:50.918267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_test_data():\n",
    "    # transform\n",
    "    mask = mlm.tokenizer.mask_token\n",
    "    all_test_data = []\n",
    "\n",
    "    for paper_id in tqdm(sample_submission['Id']):\n",
    "        # load paper\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        # extract sentences\n",
    "        sentences = set([clean_paper_sentence(sentence) for section in paper \n",
    "                         for sentence in section['text'].split('.')\n",
    "                        ])\n",
    "        sentences = shorten_sentences(sentences) # make sentences short\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "        sentences = [sentence.split() for sentence in sentences] # sentence = list of words\n",
    "\n",
    "        # mask\n",
    "        test_data = []\n",
    "        for sentence in sentences:\n",
    "            for phrase_start, phrase_end in find_mask_candidates(sentence):\n",
    "                dt_point = sentence[:phrase_start] + [mask] + sentence[phrase_end+1:]\n",
    "                test_data.append((' '.join(dt_point), ' '.join(sentence[phrase_start:phrase_end+1]))) # (masked text, phrase)\n",
    "\n",
    "        all_test_data.append(test_data)\n",
    "    return all_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff9097",
   "metadata": {
    "papermill": {
     "duration": 0.037839,
     "end_time": "2021-09-25T04:44:51.041707",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.003868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# XLM Roberta prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98979ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:51.123832Z",
     "iopub.status.busy": "2021-09-25T04:44:51.122880Z",
     "iopub.status.idle": "2021-09-25T04:44:51.125399Z",
     "shell.execute_reply": "2021-09-25T04:44:51.124990Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.751222Z"
    },
    "papermill": {
     "duration": 0.044974,
     "end_time": "2021-09-25T04:44:51.125516",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.080542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths and Hyperparameters\n",
    "MAX_LENGTH = 64 # max no. words for each sentence.\n",
    "OVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "\n",
    "PREDICT_BATCH = 64000 \n",
    "\n",
    "PRETRAINED_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/output']\n",
    "TEST_INPUT_SAVE_PATH = './input_data'\n",
    "TEST_NER_DATA_FILE = 'test_ner_input.json'\n",
    "TRAIN_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/train_ner.json']\n",
    "VAL_PATH = ['../input/coleridge-xlm-roberta-base-epoch-1-training/train_ner.json']\n",
    "\n",
    "PREDICTION_SAVE_PATH = './pred'\n",
    "PREDICTION_FILE = 'test_predictions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4de855e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:51.203109Z",
     "iopub.status.busy": "2021-09-25T04:44:51.202297Z",
     "iopub.status.idle": "2021-09-25T04:44:51.217600Z",
     "shell.execute_reply": "2021-09-25T04:44:51.218201Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.758782Z"
    },
    "papermill": {
     "duration": 0.055628,
     "end_time": "2021-09-25T04:44:51.218363",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.162735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. grouped training rows: 0\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "202d0892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:51.337033Z",
     "iopub.status.busy": "2021-09-25T04:44:51.321357Z",
     "iopub.status.idle": "2021-09-25T04:44:51.349288Z",
     "shell.execute_reply": "2021-09-25T04:44:51.350112Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.779656Z"
    },
    "papermill": {
     "duration": 0.093571,
     "end_time": "2021-09-25T04:44:51.350305",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.256734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences: 591\n"
     ]
    }
   ],
   "source": [
    "test_rows = [] # test data in NER format\n",
    "paper_length = [] # store the number of sentences each paper has\n",
    "\n",
    "for paper_id in sample_submission['Id']:\n",
    "    # load paper\n",
    "    paper = papers[paper_id]\n",
    "    \n",
    "    # extract sentences\n",
    "    sentences = [clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.')\n",
    "                ]\n",
    "    sentences = shorten_sentences(sentences) # make sentences short\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 5] # only accept sentences with length > 10 chars\n",
    "    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study', 'from'])]\n",
    "        \n",
    "    # collect all sentences in json\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split()\n",
    "        dummy_tags = ['O']*len(sentence_words)\n",
    "        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n",
    "    \n",
    "    # track which sentence belongs to which data point\n",
    "    paper_length.append(len(sentences))\n",
    "    \n",
    "print(f'total number of sentences: {len(test_rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371a8504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:51.432169Z",
     "iopub.status.busy": "2021-09-25T04:44:51.431460Z",
     "iopub.status.idle": "2021-09-25T04:44:51.433466Z",
     "shell.execute_reply": "2021-09-25T04:44:51.433868Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.836732Z"
    },
    "papermill": {
     "duration": 0.045485,
     "end_time": "2021-09-25T04:44:51.433982",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.388497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_os_env(\n",
    "    pretrained_path,\n",
    "    train_path,\n",
    "    val_path\n",
    "):\n",
    "    os.environ[\"MODEL_PATH\"] = f\"{pretrained_path}\"\n",
    "    os.environ[\"TRAIN_FILE\"] = f\"{train_path}\"\n",
    "    os.environ[\"VALIDATION_FILE\"] = f\"{val_path}\"\n",
    "    \n",
    "    os.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\n",
    "    os.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91896db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:51.514300Z",
     "iopub.status.busy": "2021-09-25T04:44:51.513578Z",
     "iopub.status.idle": "2021-09-25T04:44:52.170969Z",
     "shell.execute_reply": "2021-09-25T04:44:52.170379Z",
     "shell.execute_reply.started": "2021-09-25T01:45:38.844069Z"
    },
    "papermill": {
     "duration": 0.699155,
     "end_time": "2021-09-25T04:44:52.171095",
     "exception": false,
     "start_time": "2021-09-25T04:44:51.471940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n",
    "\n",
    "# make necessart directories and files\n",
    "os.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "284e0ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:52.254212Z",
     "iopub.status.busy": "2021-09-25T04:44:52.253379Z",
     "iopub.status.idle": "2021-09-25T04:44:52.255828Z",
     "shell.execute_reply": "2021-09-25T04:44:52.255309Z",
     "shell.execute_reply.started": "2021-09-25T01:45:39.664201Z"
    },
    "papermill": {
     "duration": 0.045987,
     "end_time": "2021-09-25T04:44:52.255935",
     "exception": false,
     "start_time": "2021-09-25T04:44:52.209948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_predict():\n",
    "    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_PATH\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --test_file \"$TEST_FILE\" \\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba96ee9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:44:52.335450Z",
     "iopub.status.busy": "2021-09-25T04:44:52.334676Z",
     "iopub.status.idle": "2021-09-25T04:46:36.879768Z",
     "shell.execute_reply": "2021-09-25T04:46:36.880185Z",
     "shell.execute_reply.started": "2021-09-25T01:45:39.670572Z"
    },
    "papermill": {
     "duration": 104.585936,
     "end_time": "2021-09-25T04:46:36.880373",
     "exception": false,
     "start_time": "2021-09-25T04:44:52.294437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Bert model 0\n",
      "rm: cannot remove './pred': No such file or directory\r\n",
      "2021-09-25 04:44:56.155485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-1569380d0ed7b188/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\r\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 9876.70it/s]\r\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 256.00it/s]\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-1569380d0ed7b188/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50. Subsequent calls will reuse this data.\r\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 86.71it/s]\r\n",
      "[INFO|configuration_utils.py:470] 2021-09-25 04:45:59,737 >> loading configuration file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-09-25 04:45:59,738 >> Model config XLMRobertaConfig {\r\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"XLMRobertaForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-05,\r\n",
      "  \"max_position_embeddings\": 514,\r\n",
      "  \"model_type\": \"xlm-roberta\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 1,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 250002\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:470] 2021-09-25 04:45:59,738 >> loading configuration file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-09-25 04:45:59,739 >> Model config XLMRobertaConfig {\r\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"XLMRobertaForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-05,\r\n",
      "  \"max_position_embeddings\": 514,\r\n",
      "  \"model_type\": \"xlm-roberta\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 1,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 250002\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-09-25 04:45:59,742 >> Didn't find file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/tokenizer.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-09-25 04:45:59,743 >> Didn't find file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/added_tokens.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-09-25 04:45:59,749 >> loading file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/sentencepiece.bpe.model\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-09-25 04:45:59,749 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-09-25 04:45:59,749 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-09-25 04:45:59,749 >> loading file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-09-25 04:45:59,749 >> loading file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/tokenizer_config.json\r\n",
      "[INFO|modeling_utils.py:1049] 2021-09-25 04:46:01,353 >> loading weights file ../input/coleridge-xlm-roberta-base-epoch-1-training/output/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:1167] 2021-09-25 04:46:22,961 >> All model checkpoint weights were used when initializing XLMRobertaForTokenClassification.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:1176] 2021-09-25 04:46:22,961 >> All the weights of XLMRobertaForTokenClassification were initialized from the model checkpoint at ../input/coleridge-xlm-roberta-base-epoch-1-training/output.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.86ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-09-25 04:46:31,968 >> The following columns in the test set  don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, tags.\r\n",
      "[INFO|trainer.py:1817] 2021-09-25 04:46:31,970 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:1818] 2021-09-25 04:46:31,970 >>   Num examples = 591\r\n",
      "[INFO|trainer.py:1819] 2021-09-25 04:46:31,970 >>   Batch size = 8\r\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [00:02<00:00, 28.20it/s]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-09-25 04:46:35,439 >> ***** test metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,439 >>   init_mem_cpu_alloc_delta  =     1279MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,439 >>   init_mem_cpu_peaked_delta =      732MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,439 >>   init_mem_gpu_alloc_delta  =     1058MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,439 >>   init_mem_gpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,439 >>   test_accuracy             =     0.9938\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_f1                   =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_loss                 =     0.0368\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_mem_cpu_alloc_delta  =       19MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_mem_gpu_alloc_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_mem_gpu_peaked_delta =       25MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_precision            =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_recall               =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_runtime              = 0:00:03.30\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-09-25 04:46:35,440 >>   test_samples_per_second   =    178.581\r\n",
      "100%|███████████████████████████████████████████| 74/74 [00:03<00:00, 22.46it/s]\r\n"
     ]
    }
   ],
   "source": [
    "final_bert_outputs = []\n",
    "for i in range(1):\n",
    "    print(f'Prediction Bert model {i}')\n",
    "    set_os_env(PRETRAINED_PATH[i], TRAIN_PATH[i], VAL_PATH[i])\n",
    "    \n",
    "    bert_outputs = []\n",
    "\n",
    "    for batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n",
    "        # write data rows to input file\n",
    "        with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n",
    "            for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n",
    "                json.dump(row, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        # remove output dir\n",
    "        !rm -r \"$OUTPUT_DIR\"\n",
    "\n",
    "        # do predict\n",
    "        bert_predict()\n",
    "\n",
    "        # read predictions\n",
    "        with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n",
    "            this_preds = f.read().split('\\n')[:-1]\n",
    "            bert_outputs += [pred.split() for pred in this_preds]\n",
    "        break\n",
    "    final_bert_outputs.append(bert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c71942d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:36.998758Z",
     "iopub.status.busy": "2021-09-25T04:46:36.997949Z",
     "iopub.status.idle": "2021-09-25T04:46:37.000559Z",
     "shell.execute_reply": "2021-09-25T04:46:37.000100Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.4346Z"
    },
    "papermill": {
     "duration": 0.063464,
     "end_time": "2021-09-25T04:46:37.000670",
     "exception": false,
     "start_time": "2021-09-25T04:46:36.937206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get test sentences\n",
    "test_sentences = [row['tokens'] for row in test_rows]\n",
    "\n",
    "del test_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14743c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:37.125884Z",
     "iopub.status.busy": "2021-09-25T04:46:37.124791Z",
     "iopub.status.idle": "2021-09-25T04:46:37.131245Z",
     "shell.execute_reply": "2021-09-25T04:46:37.130745Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.441993Z"
    },
    "papermill": {
     "duration": 0.075054,
     "end_time": "2021-09-25T04:46:37.131360",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.056306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Alzheimer s Disease Neuroimaging Initiative ADNI',\n",
       "  'Cardiovascular Health Study CHS'},\n",
       " {'Integrated Postsecondary Education Data System',\n",
       "  'NCES Common Core of Data',\n",
       "  'Progress in International Reading Literacy Study',\n",
       "  'Progress in International reading Literacy Study',\n",
       "  'Schools and Staffing Survey',\n",
       "  'Trends in International Mathematics',\n",
       "  'Trends in International Mathematics and Science Study',\n",
       "  'trends in International Mathematics and Science Study'},\n",
       " {'SLOSH model', 'Sea Lake and Overland Surges from Hurricanes'},\n",
       " set()]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset_labels = []\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "    for length in paper_length:\n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[:length], final_bert_outputs[i][:length]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == 'B': # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == 'I' and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        bert_dataset_labels.append(labels)\n",
    "\n",
    "        del test_sentences[:length], final_bert_outputs[i][:length]\n",
    "    final_dataset_labels.append(bert_dataset_labels)\n",
    "    \n",
    "final_dataset_labels[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698e556",
   "metadata": {
    "papermill": {
     "duration": 0.055331,
     "end_time": "2021-09-25T04:46:37.242949",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.187618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Filter based on Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c388f056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:37.430629Z",
     "iopub.status.busy": "2021-09-25T04:46:37.429733Z",
     "iopub.status.idle": "2021-09-25T04:46:37.438440Z",
     "shell.execute_reply": "2021-09-25T04:46:37.439379Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.466394Z"
    },
    "papermill": {
     "duration": 0.12018,
     "end_time": "2021-09-25T04:46:37.439674",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.319494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardiovascular health study chs|alzheimer s disease neuroimaging initiative adni', 'nces common core of data|schools and staffing survey|trends in international mathematics|integrated postsecondary education data system|progress in international reading literacy study', 'slosh model|sea lake and overland surges from hurricanes', '']\n"
     ]
    }
   ],
   "source": [
    "final_xlm_roberta_labels = []\n",
    "for bert_dataset_labels in final_dataset_labels:\n",
    "    filtered_bert_labels = []\n",
    "    for labels in bert_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_text(label)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.4 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_bert_labels.append('|'.join(filtered))\n",
    "    final_xlm_roberta_labels.append(filtered_bert_labels)\n",
    "del filtered_bert_labels\n",
    "\n",
    "print(final_xlm_roberta_labels[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0e1ec",
   "metadata": {
    "papermill": {
     "duration": 0.094429,
     "end_time": "2021-09-25T04:46:37.635612",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.541183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spicy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ba61506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:37.840008Z",
     "iopub.status.busy": "2021-09-25T04:46:37.839233Z",
     "iopub.status.idle": "2021-09-25T04:46:37.851263Z",
     "shell.execute_reply": "2021-09-25T04:46:37.849994Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.477718Z"
    },
    "papermill": {
     "duration": 0.119999,
     "end_time": "2021-09-25T04:46:37.851457",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.731458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:               return re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
    "def clean_texts(texts: List[str]) -> List[str]: return [ clean_text(text) for text in texts ] \n",
    "\n",
    "def read_json(index: str, test_train) -> Dict:\n",
    "    filename = f\"../input/coleridgeinitiative-show-us-the-data/{test_train}/{index}.json\"\n",
    "    with open(filename) as f:\n",
    "        json = simplejson.load(f)\n",
    "    return json\n",
    "        \n",
    "def json2text(index: str, test_train) -> str:\n",
    "    json  = read_json(index, test_train)\n",
    "    texts = [\n",
    "        row[\"section_title\"] + \" \" + row[\"text\"] \n",
    "        for row in json\n",
    "    ]\n",
    "    text  = \" \".join(texts)\n",
    "    return text\n",
    "\n",
    "def filename_to_index(filename):\n",
    "    return re.sub(\"^.*/|\\.[^.]+$\", '', filename)\n",
    "\n",
    "def glob_to_indices(globpath):\n",
    "    return list(map(filename_to_index, glob.glob(globpath)))\n",
    "\n",
    "# Inspired by: https://www.kaggle.com/hamditarek/merge-multiple-json-files-to-a-dataframe\n",
    "def dataset_df(test_train=\"test\"):\n",
    "    indices = glob_to_indices(f\"../input/coleridgeinitiative-show-us-the-data/{test_train}/*.json\")    \n",
    "    texts   = Parallel(-1)( \n",
    "        delayed(json2text)(index, test_train)\n",
    "        for index in indices  \n",
    "    )\n",
    "    df = pd.DataFrame([\n",
    "        { \"id\": index, \"text\": text}\n",
    "        for index, text in zip(indices, texts)\n",
    "    ])\n",
    "    df.to_csv(f\"{test_train}.json.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d42bf2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:37.973133Z",
     "iopub.status.busy": "2021-09-25T04:46:37.972599Z",
     "iopub.status.idle": "2021-09-25T04:46:37.984212Z",
     "shell.execute_reply": "2021-09-25T04:46:37.984727Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.490564Z"
    },
    "papermill": {
     "duration": 0.07505,
     "end_time": "2021-09-25T04:46:37.984880",
     "exception": false,
     "start_time": "2021-09-25T04:46:37.909830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "papers = {}\n",
    "for paper_id in sample_submission['Id'].values:\n",
    "    with open(f'../input/coleridgeinitiative-show-us-the-data/test/{paper_id}.json', 'r') as f:\n",
    "        sections = json.load(f)\n",
    "        paper = ''\n",
    "        for section in sections:\n",
    "            paper = paper + section['text'] + ' .'\n",
    "    papers[paper_id] = paper\n",
    "    del paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be56a6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:38.102075Z",
     "iopub.status.busy": "2021-09-25T04:46:38.101491Z",
     "iopub.status.idle": "2021-09-25T04:46:40.272983Z",
     "shell.execute_reply": "2021-09-25T04:46:40.272484Z",
     "shell.execute_reply.started": "2021-09-25T01:47:21.512803Z"
    },
    "papermill": {
     "duration": 2.230758,
     "end_time": "2021-09-25T04:46:40.273117",
     "exception": false,
     "start_time": "2021-09-25T04:46:38.042359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load spacy classifier model\n",
    "with open('../input/coleridge-spacy-classifier/spacy_model.pickle', 'rb') as f:\n",
    "    nlp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "658fdc8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:40.405583Z",
     "iopub.status.busy": "2021-09-25T04:46:40.404907Z",
     "iopub.status.idle": "2021-09-25T04:46:41.089038Z",
     "shell.execute_reply": "2021-09-25T04:46:41.089736Z",
     "shell.execute_reply.started": "2021-09-25T01:47:32.249474Z"
    },
    "papermill": {
     "duration": 0.760242,
     "end_time": "2021-09-25T04:46:41.089927",
     "exception": false,
     "start_time": "2021-09-25T04:46:40.329685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.5597970485687256 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adni|alzheimers disease neuroimaging initiative',\n",
       " 'ines|ipeds|pirls|oecd indicators national education systems|nces integrated postsecondary education data system|achievement progress international reading literacy study',\n",
       " 'apes|usgs|slosh|us geological survey|albemarle pamlico estuarine system|sea lake overland surges hurricanes',\n",
       " 'ces|consumer expenditure survey']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#### remove >.5 jaccard matches from predicitons\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = s1.split()\n",
    "    l2 = s2.split()    \n",
    "    intersection = len(list(set(l1).intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "start_time = time.time()\n",
    "column_names = [\"Id\", \"PredictionString\"]\n",
    "submission = pd.DataFrame(columns = column_names)\n",
    "\n",
    "no_delete = ['study', 'dataset', 'model','survey','data','adni','codes', 'genome', 'program','assessment','database','census','initiative','gauge','system','stewardship','surge']\n",
    "\n",
    "spacy_predictions = []\n",
    "for index, row in sample_submission.iterrows():\n",
    "    to_append=[row['Id'],'']\n",
    "    passage = papers[row['Id']]\n",
    "    passage=passage.replace(\"'s\",\"s\")\n",
    "    passage=passage.replace(\"-\",\" \")\n",
    "    passage=passage.replace(\",\",\" \")\n",
    "    \n",
    "    ######## ACRONYMS\n",
    "    for match in re.finditer(r\"(\\(([A-Z]{2,})\\))\", passage):\n",
    "    #for match in re.finditer(r\"(\\((.*?)\\))\", data):\n",
    "        caps=[]\n",
    "        start_index = match.start()\n",
    "        abbr = match.group(1)\n",
    "        size = len(abbr)\n",
    "        words = passage[:start_index].split()[-size:]\n",
    "        for word in words:\n",
    "            if word[0].isupper():\n",
    "                caps.append(word)\n",
    "        definition = \" \".join(caps)\n",
    "        if sum(1 for c in definition if c.isupper()) < 15:\n",
    "            words = [word for word in no_delete if word in definition.lower()]\n",
    "            doc=nlp(definition)\n",
    "            score=doc.cats['POSITIVE']\n",
    "            if len(words)>0 and  score > .99:\n",
    "                if to_append[1]!='' and definition not in to_append[1]:\n",
    "                    to_append[1] = to_append[1]+'|'+definition+'|'+abbr\n",
    "                    to_append[1] = to_append[1]+'|'+abbr\n",
    "                if to_append[1]=='':\n",
    "                    to_append[1] = definition\n",
    "                    to_append[1] = to_append[1]+'|'+abbr\n",
    "                            \n",
    "    #### cap word sequence\n",
    "    if to_append[1]=='':        \n",
    "        mylist=re.findall('([A-Z][\\w-]*(?:\\s+[A-Z][\\w-]*)+)', remove_stopwords(passage))\n",
    "        mylist = list(dict.fromkeys(mylist))\n",
    "        for match in mylist:\n",
    "            upper_score=sum(1 for c in match if c.isupper())\n",
    "            if upper_score < 15:\n",
    "                words = [word for word in no_delete if word in match.lower()]\n",
    "                doc=nlp(match)\n",
    "                score=doc.cats['POSITIVE']\n",
    "                if len(words)>0 and len(match.split())>=2 and score > .99:\n",
    "                    if to_append[1]!='' and match not in to_append[1]:\n",
    "                        to_append[1] = to_append[1]+'|'+match\n",
    "                    if to_append[1]=='':\n",
    "                        to_append[1] = match\n",
    "            \n",
    "    ###### remove similar jaccard\n",
    "    got_label=to_append[1].split('|')\n",
    "    filtered=[]\n",
    "    filtered_labels = ''\n",
    "    for label in sorted(got_label, key=len):\n",
    "        label = clean_text(label)\n",
    "        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < .5 for got_label in filtered):\n",
    "            filtered.append(label)\n",
    "            if filtered_labels!='':\n",
    "                filtered_labels=filtered_labels+'|'+label\n",
    "            if filtered_labels=='':\n",
    "                filtered_labels=label\n",
    "    \n",
    "    to_append[1] = filtered_labels  \n",
    "    \n",
    "    spacy_predictions.append(to_append[1])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "spacy_predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8ce43",
   "metadata": {
    "papermill": {
     "duration": 0.056342,
     "end_time": "2021-09-25T04:46:41.205980",
     "exception": false,
     "start_time": "2021-09-25T04:46:41.149638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58ff0917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:41.323281Z",
     "iopub.status.busy": "2021-09-25T04:46:41.321216Z",
     "iopub.status.idle": "2021-09-25T04:46:41.326378Z",
     "shell.execute_reply": "2021-09-25T04:46:41.325961Z",
     "shell.execute_reply.started": "2021-09-25T01:47:40.340519Z"
    },
    "papermill": {
     "duration": 0.06401,
     "end_time": "2021-09-25T04:46:41.326516",
     "exception": false,
     "start_time": "2021-09-25T04:46:41.262506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../input/ci-transformers-model-v2/model/sent_transformer'\n",
    "tokenizer_path = '../input/ci-transformers-model-v2/tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27acf0c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:41.456215Z",
     "iopub.status.busy": "2021-09-25T04:46:41.455669Z",
     "iopub.status.idle": "2021-09-25T04:46:45.724499Z",
     "shell.execute_reply": "2021-09-25T04:46:45.723522Z",
     "shell.execute_reply.started": "2021-09-25T01:47:41.557787Z"
    },
    "papermill": {
     "duration": 4.341202,
     "end_time": "2021-09-25T04:46:45.724651",
     "exception": false,
     "start_time": "2021-09-25T04:46:41.383449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 04:46:41.554988: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-25 04:46:41.556930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-25 04:46:41.582859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.583662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-09-25 04:46:41.583749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-25 04:46:41.583858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-25 04:46:41.583914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-09-25 04:46:41.583970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-25 04:46:41.584013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-25 04:46:41.584057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-25 04:46:41.584098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-09-25 04:46:41.584141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-25 04:46:41.584295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.585075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.586459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-25 04:46:41.586903: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-25 04:46:41.587142: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-25 04:46:41.587363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.588119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-09-25 04:46:41.588186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-25 04:46:41.588223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-25 04:46:41.588261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-09-25 04:46:41.588295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-25 04:46:41.588323: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-25 04:46:41.588349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-25 04:46:41.588378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-09-25 04:46:41.588406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-25 04:46:41.588530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.589275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:41.589932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-25 04:46:41.590806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-25 04:46:44.884693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-25 04:46:44.884741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-09-25 04:46:44.884758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-09-25 04:46:44.885819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:44.886750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:44.887579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-25 04:46:44.888175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14347 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" build transformer model\"\"\"\n",
    "\n",
    "maxlen = 500\n",
    "num_classes = 2\n",
    "vocab_size = 32824\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model_t = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_t.load_weights(model_path)\n",
    "model_t.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                patience=0, verbose=1, mode='auto', baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06e73657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:45.846879Z",
     "iopub.status.busy": "2021-09-25T04:46:45.846172Z",
     "iopub.status.idle": "2021-09-25T04:46:45.915332Z",
     "shell.execute_reply": "2021-09-25T04:46:45.914920Z",
     "shell.execute_reply.started": "2021-09-25T01:47:45.55333Z"
    },
    "papermill": {
     "duration": 0.132362,
     "end_time": "2021-09-25T04:46:45.915459",
     "exception": false,
     "start_time": "2021-09-25T04:46:45.783097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# prepare list of dataset titles to match\n",
    "train_df = pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\n",
    "sample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "\n",
    "ds_titles = all_datasets\n",
    "\n",
    "filtered = []\n",
    "labels = ds_titles\n",
    "for label in sorted(labels, key=len):\n",
    "    label = clean_text(label)\n",
    "    if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.2 for got_label in filtered):\n",
    "        filtered.append(label)\n",
    "        \n",
    "ds_titles = np.array(filtered)\n",
    "ds_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "520a24ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:46.040937Z",
     "iopub.status.busy": "2021-09-25T04:46:46.037622Z",
     "iopub.status.idle": "2021-09-25T04:46:48.920973Z",
     "shell.execute_reply": "2021-09-25T04:46:48.921364Z",
     "shell.execute_reply.started": "2021-09-25T01:47:45.625361Z"
    },
    "papermill": {
     "duration": 2.948999,
     "end_time": "2021-09-25T04:46:48.921524",
     "exception": false,
     "start_time": "2021-09-25T04:46:45.972525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]2021-09-25 04:46:46.190521: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-25 04:46:46.200782: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000175000 Hz\n",
      "2021-09-25 04:46:46.448722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-25 04:46:47.189284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alzheimers disease neuroimaging initiative',\n",
       " 'oecd s online education database | common core of data | program for international student assessment | progress in international reading literacy study',\n",
       " 'noaa storm surge inundation | slosh model | sea lake and overland surges from hurricanes',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ''\n",
    "with open(tokenizer_path, \"rb\") as openfile:\n",
    "    tokenizer = pickle.load(openfile)\n",
    "            \n",
    "test_data_path = '../input/coleridgeinitiative-show-us-the-data/test'\n",
    "test_sentences = {}\n",
    "candidate_threshold = 0.3\n",
    "acceptance_score = 80\n",
    "\n",
    "def read_json_pub(Id):\n",
    "    filename = os.path.join(test_data_path, Id+'.json')\n",
    "    with open(filename) as f:\n",
    "        json_pub = json.load(f)\n",
    "    return json_pub\n",
    "\n",
    "transformers_preds = []\n",
    "for index, row in tqdm(sample_submission.iterrows(), total = sample_submission.shape[0]):\n",
    "    # Load text\n",
    "    raw_text = read_json_pub(row['Id'])\n",
    "    text = '\\n'.join([z for y in raw_text for z in y.values()])\n",
    "\n",
    "    # split and clean sentences\n",
    "    sentences = nltk.sent_tokenize(re.sub(r'\\.?\\n', '. ', text))\n",
    "    sentences = [re.sub(r\"[^a-z ]+\",\"\", s.lower()) for s in sentences]\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenizer.texts_to_sequences(sentences)\n",
    "    tokens = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokens, maxlen=maxlen, padding='pre',)\n",
    "\n",
    "    # Predict candidates sentences that may contain DS references\n",
    "    y_pred = model_t.predict(tokens, batch_size=32)\n",
    "    sent_candidates = np.array(sentences)[y_pred[:,1] > candidate_threshold]\n",
    "    test_sentences[row['Id']] = sent_candidates\n",
    "\n",
    "    ds_candidates = set()\n",
    "    for sent in sent_candidates:\n",
    "        scores = [fuzz.partial_ratio(sent.lower(), title) for title in ds_titles]\n",
    "        best_fit_title_index = np.argmax(scores)\n",
    "        if max(scores) > acceptance_score:\n",
    "            ds_candidates.add(ds_titles[np.argmax(scores)])\n",
    "    prediction_string = ' | '.join(ds_candidates)\n",
    "    transformers_preds.append(prediction_string)\n",
    "    \n",
    "transformers_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170eadc",
   "metadata": {
    "papermill": {
     "duration": 0.060785,
     "end_time": "2021-09-25T04:46:49.042224",
     "exception": false,
     "start_time": "2021-09-25T04:46:48.981439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Aggregate all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b894a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:49.171207Z",
     "iopub.status.busy": "2021-09-25T04:46:49.170339Z",
     "iopub.status.idle": "2021-09-25T04:46:49.196781Z",
     "shell.execute_reply": "2021-09-25T04:46:49.197175Z",
     "shell.execute_reply.started": "2021-09-25T01:47:48.596279Z"
    },
    "papermill": {
     "duration": 0.095202,
     "end_time": "2021-09-25T04:46:49.197310",
     "exception": false,
     "start_time": "2021-09-25T04:46:49.102108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11848.32it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 10354.60it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 23467.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 23172.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adni|cardiovascular health study chs|alzheimers disease neuroimaging initiative',\n",
       " 'ines|ipeds|pirls|common core of data|schools and staffing survey|oecd s online education database|trends in international mathematics|oecd indicators national education systems|program for international student assessment|integrated postsecondary education data system|progress in international reading literacy study',\n",
       " 'apes|usgs|slosh|us geological survey|noaa storm surge inundation|albemarle pamlico estuarine system|sea lake overland surges hurricanes',\n",
       " 'ces|consumer expenditure survey|rural urban continuum codes']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions = []\n",
    "for bert_pred, trans_pred, spacy_pred, literal_match in zip(\n",
    "    final_xlm_roberta_labels[0], \n",
    "    transformers_preds, \n",
    "    spacy_predictions,\n",
    "    literal_preds\n",
    "):        \n",
    "    pred1 = [x for x in bert_pred.split('|') if x not in ['']]\n",
    "    pred2 = [x for x in trans_pred.split('|') if x not in ['']]\n",
    "    pred3 = [x for x in spacy_pred.split('|') if x not in ['']]\n",
    "\n",
    "    labels = np.unique(pred1+pred2+pred3)\n",
    "    if len(labels)>0:\n",
    "        filtered = []\n",
    "        for label in tqdm(sorted(labels, key=len)):\n",
    "            label = clean_text(label)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.4 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        final_predictions.append('|'.join(filtered))\n",
    "    else:\n",
    "        final_predictions.append(literal_match)        \n",
    "\n",
    "final_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93779d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T04:46:49.330321Z",
     "iopub.status.busy": "2021-09-25T04:46:49.329523Z",
     "iopub.status.idle": "2021-09-25T04:46:49.340528Z",
     "shell.execute_reply": "2021-09-25T04:46:49.340075Z",
     "shell.execute_reply.started": "2021-09-25T01:47:48.628872Z"
    },
    "papermill": {
     "duration": 0.080511,
     "end_time": "2021-09-25T04:46:49.340639",
     "exception": false,
     "start_time": "2021-09-25T04:46:49.260128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>adni|cardiovascular health study chs|alzheimer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td>ines|ipeds|pirls|common core of data|schools a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>apes|usgs|slosh|us geological survey|noaa stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>ces|consumer expenditure survey|rural urban co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466   \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1   \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  adni|cardiovascular health study chs|alzheimer...  \n",
       "1  ines|ipeds|pirls|common core of data|schools a...  \n",
       "2  apes|usgs|slosh|us geological survey|noaa stor...  \n",
       "3  ces|consumer expenditure survey|rural urban co...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['PredictionString'] = final_predictions\n",
    "sample_submission[['Id', 'PredictionString']].to_csv('submission.csv', index=False)\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50dc453",
   "metadata": {
    "papermill": {
     "duration": 0.06446,
     "end_time": "2021-09-25T04:46:49.469990",
     "exception": false,
     "start_time": "2021-09-25T04:46:49.405530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 233.733899,
   "end_time": "2021-09-25T04:46:52.470609",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-25T04:42:58.736710",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
